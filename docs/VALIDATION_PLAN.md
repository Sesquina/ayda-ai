# Ayda AI â€” Validation & User Testing Plan

Version 1.0 Â· November 2025

---

### 1ï¸âƒ£ Objective

Validate Aydaâ€™s ability to improve **comprehension, confidence, and care coordination** among caregivers and patients reviewing medical records.

The plan measures *clinical literacy outcomes* and *usability metrics* across diverse users while ensuring full ethical and IRB compliance.

---

### 2ï¸âƒ£ Study Design

| Parameter      | Description                                                    |
| -------------- | -------------------------------------------------------------- |
| **Design**     | Mixed-methods: pre/post comprehension tests + in-app feedback  |
| **Population** | 60 caregivers (bilingual cohort: 30 English / 30 Spanish)      |
| **Setting**    | Remote, virtual testing via HIPAA-secure Ayda app              |
| **Duration**   | 4 weeks per cohort                                             |
| **Goal**       | Quantify comprehension gain, stress reduction, and trust score |

---

### 3ï¸âƒ£ Methods

#### a. Quantitative Metrics

1. **Comprehension Accuracy (%)** â€” pre/post-test based on mock discharge summaries.
2. **Reading Time (seconds)** â€” average time per summary.
3. **Red Flag Recognition (%)** â€” % of users correctly identifying urgent issues.
4. **Feedback Sentiment (ğŸ‘ / ğŸ‘ ratio)** â€” real-time satisfaction indicator.
5. **COMPREHENSION_SCORE Î” (Firestore metric)** â€” automatic internal scoring trend.

#### b. Qualitative Metrics

1. **Emotional Tone Feedback** â€” 1â€“5 scale (â€œCalm,â€ â€œOverwhelming,â€ etc.)
2. **Trust in Summary (Likert 1â€“5)** â€” â€œI feel confident explaining this to my doctor.â€
3. **Ease of Use (SUS score)** â€” System Usability Scale administered at end of week 4.

---

### 4ï¸âƒ£ Data Collection & Analysis

* **Firestore metrics:** auto-logged by Ayda backend.
* **Survey responses:** stored securely in `/feedback` collection.
* **Analysis tool:** exported to Python notebook for paired t-tests + effect size.
* **Target effect size:** *Cohenâ€™s d â‰¥ 0.6* (moderateâ€“strong comprehension improvement).

---

### 5ï¸âƒ£ Ethical Oversight

* De-identification pipeline enforced before AI access.
* No PHI retained post-summary.
* IRB consultation planned via NIH Applicant Assistance Program (AAP).
* Consent form stored as digital acknowledgment within app.
* Participants can delete data any time (GDPR-compliant logic).

---

### 6ï¸âƒ£ Success Criteria (for SBIR Phase I)

| Metric              | Threshold | Interpretation                          |
| ------------------- | --------- | --------------------------------------- |
| Avg Comprehension â†‘ | â‰¥ +30 %   | Statistically significant literacy gain |
| Red Flags â†“         | â‰¥ -40 %   | Earlier recognition of urgent issues    |
| SUS Score           | â‰¥ 80/100  | â€œExcellentâ€ usability                   |
| Trust Score         | â‰¥ 4.0/5   | Emotional validation                    |
| Attrition           | â‰¤ 15 %    | Strong engagement                       |

---

### 7ï¸âƒ£ Deliverables

* **Validation Summary Report (PDF)** â€” for SBIR Phase II submission
* **Poster-ready visualizations** â€” comprehension and stress metrics
* **IRB statement of compliance** â€” included in Phase I completion packet

---

## âœ… Next Action

After MVP deployment, recruit 10 pilot users from bilingual caregiving networks (e.g., Latino Cancer Institute, CaringBridge).
Run the 4-week pilot â†’ export anonymized results â†’ attach to grant report.
